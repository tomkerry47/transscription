Activating virtual environment...
Starting Whisper server with large-v3-turbo model...
INFO:__main__:âœ… CUDA available: NVIDIA GeForce RTX 3050
INFO:__main__:âœ… CUDA version: 11.8
INFO:__main__:ğŸ¯ Model: large-v3-turbo
INFO:__main__:â±ï¸ Phrase timeout: 3.0s
INFO:__main__:ğŸš€ Using large-v3-turbo for highest quality and speed!
INFO:__main__:Initializing SHARED Whisper model for all clients...
INFO:__main__:Loading SHARED Whisper model 'large-v3-turbo' on device: cuda
INFO:__main__:âœ… SHARED Whisper model 'large-v3-turbo' loaded successfully
INFO:__main__:Real-time WebSocket server initialized with SHARED large-v3-turbo model
INFO:__main__:Phrase timeout set to 3.0s for all future connections
INFO:__main__:Starting real-time Whisper WebSocket server v3 on 0.0.0.0:8000
INFO:websockets.server:server listening on 0.0.0.0:8000
INFO:__main__:âœ… Real-time Whisper server v3 listening at ws://0.0.0.0:8000/ws/transcribe
INFO:__main__:âœ… Server ready for real-time transcription with self-correction
INFO:__main__:ğŸš€ Using SHARED model - clients connect instantly without model loading
INFO:__main__:âœ… Using model on device: NVIDIA GeForce RTX 3050
INFO:__main__:âœ… Phrase timeout: 3.0s
INFO:__main__:ğŸš€ Real-time server v3 OPTIMIZED started successfully! Press Ctrl+C to stop.
INFO:__main__:ğŸ”„ Server will provide real-time updates with self-correction
INFO:__main__:âš¡ OPTIMIZED: Using SHARED model - clients connect instantly!
INFO:__main__:ğŸ¯ PERFORMANCE: Single model load, multiple clients supported
INFO:__main__:ğŸ‘¥ Each client connection gets its own isolated transcriber
INFO:websockets.server:connection open
INFO:__main__:Real-time transcriber initialized for client 10.115.1.166:52179-1 with 3.0s phrase timeout
INFO:__main__:Client connected: 10.115.1.166:52179-1 (Using SHARED model, no loading needed)
INFO:__main__:Starting real-time transcription for 10.115.1.166:52179-1
INFO:__main__:Transcriber session reset for 10.115.1.166:52179-1 - cleared all previous data
